---
title: "AI: Evaluating an ML Model"
author: "DAIM Team"
format: revealjs
logo: /media/ldd-logo.png
css: /slides.css
title-slide-attributes:
  data-background-image: /media/daim/bg_cubes_red.png
  data-background-opacity: "0.6"
---

## Module 4: Part 2 Learning Objectives {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- Why **evaluate** ML models and understand **key metrics**
- Understand **pros** and **cons** of different metrics
- Understand the **limitations** of different metrics

## You fitted a model - now what? {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- What do you do next?
  - Depends on why you are building it for.
- Consider
  - How do we get people to **use** this model
  - How do we **generalise the findings** to other datasets
  - How do we have it **approved** by peers and medico-legally

#

:::{.r-stack}
**How to evaluate a machine learning model**
:::

## Complexity of the Model {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- In ML, you need to trade off between **bias** and **variance**:
  - Very biased model = assuming everyone is sick
  - High variant model = responding to noise
- This describes the difference between **overfitting** and **underfitting**

![](/media/daim/module_4/biasvariance.jpg)[Source: [1]]

## Overfitting {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- The more **parameters** a model has, the more **information** it can represent
- However, this can lead to **overfitting** and **inefficiency**

![](/media/daim/module_4/biasvariance.jpg)[Source: [1]]

## Evaluate Your Model: How? {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- Validation is **key** to **model reliability**.
  - **Validation set** = marking your own homework!
  - **Test dataset** = using a separate, fresh dataset
- Beware of **data leakage**
  - _We will return to this later..._

## Theoretical: Why evaluate models? {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- **Models are not truth**.
  - They **apporoximate** the real world.
- As a good scientist, you should have a **healthy skepticism** of your hypotheses
  - In extension, you should be skeptical of models you fitted.

## Null hypothesis and Model Evaluation: parallels {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- Hypothesis testing: is our finding '**by chance**'?
  - **Null Hypothesis**: First assume there is **no effect**.
- Similarly, we will start from evaluating **naive** model
  - We will **compare** our models against **naive** models.
- This allow us to **generalise** our model findings.

## Generalisation in models {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- **Generalisation** means the findings in **your dataset** are not **memorised** by the model.
- This means your model findings are **applicable** in other settings.
- It works in **external data** that is **unseen**.

## What makes a good model? {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- A good model is **generalisable** and therefore **applicable**.
- Note: A well-performing model in one setting **may not work in others.**
  - e.g. **screening** test for cancer vs. **confirmation** test.

#

:::{.r-stack}
**Different types of prediction**
:::

## Regression vs. classification {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- **Regression**
  - How do multiple **independent** variables affect a **depedent** variable?
- Classification
  - Does an **outcome** fit into a certain **class** or not?

## Theory: Residuals {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- Difference between **true value** and **predicted value** is called a **residual**.
  - **Residuals** can be positive or negative.
- In this example, the dotted lines are residuals.

![](/media/daim/module_4/residuals.png)[Source: [2]]{width=80%}

## Theory: Residuals {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- Intuitively, the **worse** your model is, the **bigger** your **residuals** would be.
  - What is the **worst** model?

![](/media/daim/module_4/residuals.png)[[Source [2]]]{width=80%}

## The naive model {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- The worst model is your naive model.
  - Predicting the **mean average** at **every value** will give the **largest residuals**.
  - This is your **null model**.

![](/media/daim/module_4/mean_regression.png)

## Better models {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- For regression, you want a model that has the **smallest residual**.
  - But _how_ small?

## Saturated Model {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- What is a **saturated model**?
- A theoretical model where there are as many **parameters** as **data points**.
- i.e. You have 5 pairs of measurements for 5 variables.
  - If you use all 5, that is a **saturated** model.

![](/media/daim/module_4/saturated_regression.png)

## Saturated vs null models {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- Worst model (residuals = max) → very **biased**
- "saturated" model (residuals = 0) → very **variant**.
  - Why do you not want a **saturated** model?

## Overfitting {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- A **saturated** model means it is **overfitted**.
- Therefore, you want **a balance** between a naive model and saturated model.

## Comparing the models {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

![](/media/daim/module_4/null.png)[Source: [3]]

#

:::{.r-stack}
**How can we quantify how good a prediction is?**
:::

## Metrics

- We must use a **metric** for evaluating the model's predictions.

## Absolute Error {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

1. Absolute Error, where $y_i$ is actual value and $y_p$ is predicted value.

$$
AE = |y_i - y_p|
$$

## Mean absolute error (MAE) {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- Divide the sum of the differences between each **prediction** and the **actual value** by the **number of predictions**.

$$
MAE = \frac{1}{n} * \sum_{i=1}^{n}(y_i - y_p)
$$

## Mean squared error {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

$$
MSE = \frac{1}{n} * \sum_{i=1}^{n}(y_i - y_p)^2
$$

- Take the absolute errors and square them.
- **Add them together** and **divide** by the sample size (mean average).
- The square root can be taken of this value to give the Root MSE (**RMSE**)

## Summary: Basic regression metrics {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- **Mean Absolute Error** = mean absolute differences
- **Mean Squared Error** = measures the variance of the residuals.
- **Root Mean Squared Error** = measures standard deviation of the residuals

#

:::{.r-stack}
**Moving to classification**
:::

## Moving to classification {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- Moving from **residuals** to **deviance.**
  - Residual is specific to when **y is continuous**.
  - **Deviance** is a generalised term for residual.
- **Deviance** quantifies the **difference** between model and observations.

## Moving to classification {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- In classification problems, the **ground truth** is **0** or **1**.
  - Pneumonia / no pneumonia
- The model will produce a **probability** that the example belongs to a **class**
  - **0.03 = low probability of pnuemonia**
- How can we evaluate a **classification** model with the metrics we have discussed?

## Overall metrics {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- In **regression**, we use MSE and RMSE
- In classification, we use **log loss**
  - These are the key most important metrics.
- We use **log-loss** as our loss function to **train our model** in the last workshop.
  - The log-loss shows how well the model predicts the **probability** of a **binary outcome**

## Extending to Crossentropy Loss {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- We have been disccussing classification for **one class**.
  - E.g. cat vs dog.
  - This is **binary classification**
- How about **multiple** classes?
  - e.g. car vs cat vs dog.
- Here we generalise from **log loss** to **crossentropy loss.**

## Break! {background-image="/media/daim/coffee1.png" background-opacity="0.6" text-align=center}

#

:::{.r-stack}
**Going beyond log-loss**
:::

## Discrete metrics {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- Models will output a **probability for each class**.
  - Probabilities are **continuous** (e.g. 0.2, 0.92).
- To make a prediction (pneumonia/no pneumonia), we need to set a **threshold**
  - Where we set the threshold will affect the classification **performance.**
- This threshold usually defaults to **0.5**.
  - This is what we see when Keras shows us the **accuracy** of our model.

## Confusion Matrix {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

|                         | Model Predicted as _Yes_ | Model Predicted as _No_ |
| ----------------------- | ------------------------ | ----------------------- |
| **True value is _Yes_** | A                        | B                       |
| **True value is _No_**  | C                        | D                       |

_True Positives = A_

_True Negatives = D_

_False Positives = C_

_False Negatives = B_

## Confusion Matrix {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

|                     | Model Predicted as _Yes_ | Model Predicted as _No_ |
| ------------------- | ------------------------ | ----------------------- |
| True value is _Yes_ | A                        | B                       |
| True value is _No_  | C                        | D                       |

_Total sample size = A + B + C + D_

_Total cases = A + B_

_Total not-cases = C + D_

_Prevalence = total cases / total sample size_

## Exercise {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

|          | Model = 1 | Model = 0 |
| -------- | --------- | --------- |
| True = 1 | 80        | 30        |
| True = 0 | 20        | 40        |

- Work out the following:
  - Total Sample size, Total Cases, Prevalence
  - Accuracy

## Metrics beyond accuracy... {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- What are the **limitations** with accuracy?
- Other metrics give us a more **holistic** assessment of how the classifier works:
  - _Precision, specificity, recall/sensitivity_

## Balancing evaluation {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- The F1 score
  - The **(harmonic) mean average** of the precision and recall.
- Gives an idea of the **overall predictive performance** of the classifier.

## What happens if we change the threshold? {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- As discussed earlier, many models predict **probabilities** which are **continuous** values.
  - **Classification** relies on **discrete**, **binary** predictions. (Yes/No)

## What happens if we change the threshold? {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- How can we evaluate how the prediction of the model changes as we **modify the threshold** for detection?
- This works for **clinical tests** too.
  - What level of procalcitonin best predicts bacterial infection?

## Area under Curve

- **Sensitivty** and **specificity** is a trade off.
- If you plot the **sensitivty** against the **specificity** for different thresholds, a **receiver-operator curve (ROC)** can be generated.
- The area under this curve (AUC) gives us information about **classification performance**.
  - 0.5 indicates **random guessing**, and 1.0 is **perfect classification performance**.

## Example of AUC curve {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

![An ROC curve which shows the classification performance for blood different infection markers for predicting serious bacterial infection (SBI) in febrile infants. *Milcent K, Faesch S, Gras-Le Guen C, et al. Use of Procalcitonin Assays to Predict Serious Bacterial Infection in Young Febrile Infants. JAMA Pediatr. 2016;170(1):62–69. doi:10.1001/jamapediatrics.2015.3210*](/media/daim/module_4/milcent_et_al_ROC.png)

#

:::{.r-stack}
**Putting it all together**
:::

## Evaluating models {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- You now know how to **fit** models.
- You now know how to **evaluate** the models.
- What do you **evaluate** it on?
  - The gold standard is an **external data set**.
- Short of that:
  - Split your data into **training** and **test** set.

## Train-Test Split {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- Test set is **unseen data** for your model.
- You will **report performance** on test set
- Test set most closely **resembles** an external independent dataset.
  - But, **_data leakage_**

## What is data leakage? {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- Data leakage is where you have **ridiculously good results**.
- It is a **false discovery**.
- Involves **information being available** to the model in training that **it will not have access to** in deployment.
- Data leakage can happen for **many reasons.**

## Examples of data leakage {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- [This paper](https://arxiv.org/pdf/1711.05225) trained a chest XR classifier to predict pneumonia.
- They split 112k XRs from 31k different patients randomly into training and validation sets.
- **Where is the issue here**?

## Examples of data leakage {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- The model could **learn patient anatomy** and changes between each XR, **rather than detecting pneumonia**.
- [This was noticed by another researcher](https://web.archive.org/web/20180610093107/https://twitter.com/nizkroberts/status/931121395748270080) and **subsequently corrected**.
  - The authors subsequently made sure that there was **no patient crossover** between the datasets.

## Break! {background-image="/media/daim/coffee1.png" background-opacity="0.6" text-align=center}

#

:::{.r-stack}
**Practical and Ethical Considerations**
:::

## Scenario 1 {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- You are a **GP**.
- You use an **AI-based clinical decision support(CDS) tool** to help manage patients with "breast lump" presentations.
- The tool helps decide amongst options: **invasive investigation**, **imaging**, or **watchful waiting**.
- **A 17-year-old male patient** presents with a "breast lump."
- Should you _trust_ the model recommendations for this patient?

## Scenario 2 {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- You are a **clinical lead** for your organization.
- You want to improve pathways for acutely unwell patients.
- You have a **£100,000** budget for this.
- Your team presents two options:
  - Option 1: Recruit **two specialist nurses**
  - Option 2: Deploy an **AI-based CDS and risk management system**

## Gold Standards in Medical AI {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- Refer to [this BMJ article on standards](https://www.bmj.com/content/384/bmj-2023-074821)
- Also, see [related BMJ content](https://www.bmj.com/content/384/bmj-2023-074819)

![](/media/daim/module_4/extvali.jpg)

## Ethics of AI {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- The ethics of AI/ML are complex:
  - Should **generative AI models** (like ChatGPT) be allowed in research writing?
  - Should clinical judgments **factor in** AI recommendations?
- **AI/ML don’t always get it right**, but neither do humans
  - Does AI/ML need to be **better** than humans?
  - **Which humans?** FY1 or consultant?

## How Do We ~~Do Good~~ _Stop Bad_ AI? {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- Recognize inherent **biases** and **discrimination**
- **Assume bias is present**, then **find** and **address** it
- Maximize **individual autonomy** and **privacy**
- **Reproducibility** and **transparency** are crucial

![](/media/daim/module_4/turingway.jpg)

## Even if AI is Good? {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

![](/media/daim/module_4/inequality.jpeg){#fig-inequality}

**NHS app and deprivation:** See [this study](https://informatics.bmj.com/content/30/1/e100809)

## Reproducibility: An Analogy {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

::: {#fig-elephants layout-ncol=2}

![](/media/daim/module_4/kitchen.jpg){#fig-kitchen}

![](/media/daim/module_4/recipe.jpg){#fig-recipe}

**Kitchen and Recipe**

Source: [Kitchen image](https://www.fusespecialtyappliances.com/blog/appliance-stores-boca-raton), [Recipe](https://www.bbcgoodfood.com/recipes/classic-scones-jam-clotted-cream)

:::

## Reproducibility: Important Questions {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- **Points to consider:**
  - What type of oven?
  - What mode: fan or gas?
  - Which oil: olive oil or rapeseed?
  - Type of milk?
- In ML, consider:
  - **Technical details** like **Python version** and **framework**
  - **Clinical details** like **film projection** and **sample types**

## Real-Life Example {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

![](/media/daim/module_4/neupane_table1.png)

**Different operating systems parse data differently**
Reference: Bhandari Neupane et al., "Characterization of Leptazolines a–d" (2019).

---

## Reproducibility Analogy {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

![](/media/daim/module_4/repro.jpg)[Source: [Kitchen image](https://www.fusespecialtyappliances.com/blog/appliance-stores-boca-raton)]

---

## Maximising Reproducibility {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- **Aim for reproducibility**, even if it is **challenging** at first.
- Visit [**RAPS with R**](https://raps-with-r.dev/) for reproducibility principles.

#

:::{.r-stack}
**Revisiting the scenarios**
:::

## Scenario 1 Revisited {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- You are a **GP**.
- You use an **AI-based clinical decision support(CDS) tool** to help manage patients with "breast lump" presentations.
- The tool helps decide amongst options: **invasive investigation**, **imaging**, or **watchful waiting**.
- **A 17-year-old male patient** presents with a "breast lump."
- **Question**: Should you _trust_ the model's recommendations for this patient?

## Scenario 1: Critical Questions {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- Was the AI CDS trained on **both male and female** images?
- Were **patients under 18** included in the training set?
- Is the model optimized for **sensitivity** or **specificity**?
- What does your **clinical judgment** suggest?
- What are the **local governance policies** regarding AI use?

## Scenario 2 Revisited {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- You are a **clinical lead** for your organization.
- You want to improve pathways for acutely unwell patients.
- You have a **£100,000** budget for this.
- Your team presents two options:
  - Option 1: Recruit **two specialist nurses**
  - Option 2: Deploy an **AI-based CDS and risk management system**
- **Question**: Which option would you choose?

## Scenario 2: Considerations {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- **Nurses**:
  - Is the budget **adequate** for the clinical support that is needed?
  - Would they provide **versatility** and fill in during staff shortages?

## Scenario 2: Considerations {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- **AI System**:
  - Does your organization have the **digital infrastructure** for AI?
  - Are **clinical pathways aligned** with AI integration?
  - If AI flags deterioration, can your pathways **respond effectively**?
  - AI offers scalability and continuous availability, but **what if it fails**?

#

:::{.r-stack}
**Conclusion**
:::

## Extending the workshop task {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- The workshop task demonstrates **first principles** and provides a **introduction into building a machine learning model**.
- Machine learning is a **broad field**, with more fields than just **computer vision**...

## What we have covered {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- Today, we have covered:
  - The basics of **regression** and **classification**
  - Comprehensively **evaluating** binary classifiers
  - **Ethical** and **practical** considerations of AI in clinical settings
- We will delve into these further in the **second part** of the **workshop**.

## Further Reading and Resources {background-image="/media/daim/bg_cubes_red_right.png" background-opacity="0.6"}

- For reproducibility guidelines, see [**The Turing Way**](https://the-turing-way.netlify.app/)
- More on big data ethics: [**Data Ethics Framework**](https://www.gov.uk/government/publications/data-ethics-framework)

## Discussion and Q&A {background-image="/media/daim/coffee1.png" background-opacity="0.6" text-align=center}

- Let’s discuss any remaining questions on ML/AI in clinical applications!

## Acknowledgements {background-image="/media/daim/coffee1.png" background-opacity="0.6" text-align=center}

Thank you to Teddy Hla for writing these slides.

## Sources

[1] - UNDERFIT and OVERFIT Explained. Aarthi Kasirajan, Medium. URL: https://medium.com/@minions.k/underfit-and-overfit-explained-8161559b37db

[2] - How to interpret residuals in a linear regression model. Crystal X, Medium. URL: https://tracyrenee61.medium.com/how-to-interpret-residuals-in-a-linear-regression-model-7cfd8141d456

[3] - Saturated Models, Deviance and the Derivation of Sum of Squares. Egor Howell, Medium. URL: https://towardsdatascience.com/saturated-models-deviance-and-the-derivation-of-sum-of-squares-ee6fa040f52/
