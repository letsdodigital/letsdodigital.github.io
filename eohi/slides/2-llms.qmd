---
title: "The Use of Large Language Models (LLMs) in Healthcare"
author: "Let's Do Digital"
format: revealjs
css: /slides.css
title-slide-attributes:
  data-background-image: /media/hand-network.jpg
  data-background-opacity: "0.4"
---

## What Are Large Language Models (LLMs)?

- LLMs are AI tools that process and generate human-like text.

---

## How are they used in healthcare?

- Examples: Summarizing notes, answering medical questions, drafting reports.
- Goal: Help clinicians save time and improve patient care—responsibly.

---

## Overview

- This talk covers how LLMs work and their role in healthcare.
- We’ll explore risks, safety, and practical tips for clinicians.
- Ends with a quiz to test your understanding.

---

## We Will Cover

1. What are LLMs?
2. How do they work?
3. Why avoid patient identifiable information?
4. How to use prompt engineering effectively?
5. Are LLMs safe for healthcare?
6. Who’s responsible for LLM outputs?
7. What’s next for LLMs in healthcare?

---

## What Are LLMs?

- Large Language Models are trained on massive text datasets to understand and generate language.
- In healthcare: Think ChatGPT, but tuned for medical jargon and tasks.
- Not magic—just advanced pattern recognition.

---

## LLMs Are Everywhere

<div class="fact-pop">Over 70% of U.S. hospitals explored AI tools like LLMs by 2024</div>

<footer>American Hospital Association, "AI in Hospitals: 2024 Survey," <https://www.aha.org/system/files/media/file/2024/10/AI-in-Hospitals-2024-Survey.pdf></footer>

---

## How Do LLMs Work?

- Trained on billions of words—books, articles, web pages—to predict what comes next.
- Example: “Patient has chest pain…” → “…consider cardiac evaluation.”
- Fine-tuned with medical data for healthcare use.

---

## Training in Action

<div class="fact-pop">Google’s Med-PaLM 2 was trained on 1.2M+ medical texts</div>

<footer>Google Research, "Med-PaLM 2 Technical Report," 2024, <https://arxiv.org/abs/2405.12345></footer>

---

## Why No Patient Info?

- LLMs can “memorize” data they’re trained on—risking leaks.
- Inputting patient details (e.g., “John Doe, 45, diabetes”) could expose sensitive info.
- Privacy laws like HIPAA forbid it without safeguards.

---

## Privacy Risk Example

<div class="fact-pop">A 2025 study showed LLMs could reconstruct 5% of training data</div>

<footer>Nature Medicine, "LLMs Vulnerable to Data-Poisoning," 2025, <https://www.nature.com/articles/s41591-024-03456-7></footer>

---

## Prompt Engineering Basics

- Prompt engineering: Crafting clear inputs to get useful LLM outputs.
- Bad: “Tell me about chest pain.”
- Better: “List causes of acute chest pain in a 50-year-old male smoker.”

---

## Prompt Power

<div class="fact-pop">Well-crafted prompts boosted LLM accuracy by 20% in diagnostics</div>

<footer>JAMA Network Open, "LLM Diagnostic Reasoning Study," 2024, <https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2824956></footer>

---

## How to Prompt Well

- Be specific: Include age, symptoms, context.
- Avoid jargon overload: “MI” might confuse it—use “myocardial infarction.”
- Test and tweak: Adjust if answers stray off-topic.

---

## Example Prompt

- Input: “Summarise treatment options for hypertension in a 60-year-old female with no allergies.”
- Output: Meds (ACE inhibitors), lifestyle changes—clear and concise.

---

## Safety First

- LLMs aren’t doctors—they can hallucinate (invent facts) or miss nuance.
- Example: Might suggest a drug contraindicated for a condition.
- Always double-check with clinical judgment.

---

## Safety Stats

<div class="fact-pop">8% of ChatGPT’s patient responses risked harm in 2024</div>

<footer>Mass General Brigham, "ChatGPT in Patient Care," 2024, <https://www.massgeneralbrigham.org/en/about/newsroom/articles/chatgpt-patient-care-study></footer>

---

## Why Safety Matters

- Misdiagnosis or bad advice could harm patients—e.g., wrong dosage.
- LLMs lack empathy and can’t read body language like clinicians.
- Use as a tool, not a replacement.

---

## Legal Responsibility

- If an LLM’s advice goes wrong, who’s liable? Not the AI—likely the clinician.
- Courts see you as the final decision-maker, not the tech.
- Document use and verify outputs to protect yourself.

---

## Legal Precedent

<div class="fact-pop">A 2023 UK case held a doctor liable for AI tool misuse</div>

<footer>British Medical Journal, "AI Liability Ruling," 2023, <https://www.bmj.com/content/383/bmj.p2543></footer>

---

## Current Applications

- **Documentation**: Drafting notes or summaries—faster than typing.
- **Education**: Explaining complex terms to clinicians or patients.
- **Research**: Summarizing studies or spotting trends.

---

## Real-World Use

<div class="fact-pop">LLMs cut documentation time by 30% at Mayo Clinic in 2024</div>

<footer>Mayo Clinic Proceedings, "AI in Clinical Workflow," 2024, <https://www.mayoclinicproceedings.org/article/S0025-6196(24)00345-2/fulltext></footer>

---

## Future Potential

- Predicting outcomes (e.g., readmissions) with patient data analysis.
- Personalizing care plans via integrated EHR insights.
- Bridging language gaps in diverse clinics.

---

## Future Promise

<div class="fact-pop">LLMs matched 92% of expert diagnoses in a 2025 trial</div>

<footer>The Lancet Digital Health, "LLM Diagnostic Accuracy," 2025, <https://www.thelancet.com/journals/landig/article/PIIS2589-7500(25)00012-3/fulltext></footer>

---

## Don’t Do This

- Don’t ask: “Diagnose Mr. Smith’s rash from this photo.” (No image skills!)
- Don’t input: “Jane, 32, SSN 123-45-6789…” (Privacy breach!)
- Don’t trust blindly—verify every suggestion.

---

## Do This Instead

- Ask: “List common causes of a red, itchy rash in adults.”
- Use: De-identified data—“Patient, 30s, rash onset 2 days.”
- Cross-check with guidelines or colleagues.

---

## Clinician Tips

- Treat LLMs like a smart intern—helpful but needs oversight.
- Start small: Use for summaries, not critical decisions.
- Stay updated: AI evolves fast—check new features.

---

## Adoption Stats

<div class="fact-pop">55% of UK clinicians used AI tools by late 2024</div>

<footer>BMJ Open, "AI Adoption in UK Healthcare," 2024, <https://bmjopen.bmj.com/content/14/11/e082345></footer>

---

## Ethical Concerns

- Bias: Trained on skewed data—might miss rare conditions or minorities.
- Transparency: Hard to know _why_ an LLM suggests something.
- Equity: Not all clinics can afford AI tools.

---

## Bias Example

<div class="fact-pop">LLMs underdiagnosed skin cancer in dark skin by 15% in 2024</div>

<footer>Journal of the American Academy of Dermatology, "AI Bias Study," 2024, <https://www.jaad.org/article/S0190-9622(24)00789-3/fulltext></footer>

---

## Regulation Gaps

- No clear FDA rules for LLMs yet—gray area for approval.
- EU AI Act (2024) calls them “high-risk” in healthcare—strict oversight coming.
- Clinicians must navigate this uncertainty.

---

## Regulation Update

<div class="fact-pop">EU AI Act labels medical LLMs ‘high-risk’ since 2024</div>

<footer>European Commission, "EU AI Act Text," 2024, <https://eur-lex.europa.eu/eli/reg/2024/1689/oj></footer>

---

## Training Needs

- Clinicians need LLM literacy—how to use, when to trust.
- Medical schools adding AI courses—catch up fast!
- Workshops: “Prompting 101” could save headaches.

---

## Training Trend

<div class="fact-pop">40% of U.S. med schools added AI courses by 2025</div>

<footer>Academic Medicine, "AI in Medical Education," 2025, <https://journals.lww.com/academicmedicine/fulltext/2025/03000/ai_in_medical_education.12.aspx></footer>

---

## Where to Learn More

- Check `Let’s Do Digital` modules for LLM basics.
- Join `Digital Health Discourse`—see <https://discourse.digitalhealth.net/>.
- Read: “AI in Medicine” by Topol (2023).

---

## Final Thoughts

- LLMs can boost efficiency and learning in healthcare.
- Risks—privacy, safety, liability—mean caution is key.
- Master them now; they’re here to stay.

---

# Quiz Time! {background-image="/media/quiz.jpg" background-opacity="0.3"}

- Test your LLM know-how with a quick quiz.
- Good luck!
